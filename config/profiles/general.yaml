# General Agent Profile
# A versatile configuration for general-purpose agent task execution
# Includes web browsing, code execution, file operations, and fundamental tools

# Default agent configurations
agents:
  # Atomizer: Fast decision-making with GLM models
  atomizer:
    llm:
      model: openai/glm-4.5-air
      temperature: 0.0
      max_tokens: 8000
    # Load atomizer seed prompt and demos for improved task classification
    signature_instructions: "prompt_optimization.prompts.seed_prompts.atomizer_seed:ATOMIZER_PROMPT"
    demos: "prompt_optimization.prompts.seed_prompts.atomizer_seed:ATOMIZER_DEMOS"

  # Planner: Strategic planning with GLM models
  planner:
    llm:
      model: openai/glm-4.5-air
      temperature: 0.1
      max_tokens: 16000
    # Load planner seed prompt and demos for improved task decomposition
    signature_instructions: "prompt_optimization.prompts.seed_prompts.planner_seed:PLANNER_PROMPT"
    demos: "prompt_optimization.prompts.seed_prompts.planner_seed:PLANNER_DEMOS"
    agent_config:
      max_subtasks: 8

  # Default Executor (fallback for unmapped task types)
  executor:
    llm:
      model: openai/glm-4.6
      temperature: 0.2
      max_tokens: 32000
    prediction_strategy: "chain_of_thought"
    # Load executor seed prompt and demos for improved task execution
    signature_instructions: "prompt_optimization.prompts.seed_prompts.executor_seed:EXECUTOR_PROMPT"
    demos: "prompt_optimization.prompts.seed_prompts.executor_seed:EXECUTOR_DEMOS"
    agent_config:
      max_executions: 5
    agent_config:
      max_subtasks: 15

  # Aggregator: Synthesis with GLM models
  aggregator:
    llm:
      model: openai/glm-4.5-air
      temperature: 0.0
      max_tokens: 32000
    # Load aggregator seed prompt for improved result synthesis
    signature_instructions: "prompt_optimization.prompts.seed_prompts.aggregator_seed:AGGREGATOR_PROMPT"
    demos: "prompt_optimization.prompts.seed_prompts.aggregator_seed:AGGREGATOR_DEMOS"

  # Verifier: Validation with GLM models
  verifier:
    llm:
      model: openai/glm-4.5-air
      temperature: 0.0
      max_tokens: 16000
    # Load verifier seed prompt and demos for improved output validation
    signature_instructions: "prompt_optimization.prompts.seed_prompts.verifier_seed:VERIFIER_PROMPT"
    demos: "prompt_optimization.prompts.seed_prompts.verifier_seed:VERIFIER_DEMOS"

# Task-aware agent mapping for executor
agent_mapping:
  executors:
    # RETRIEVE: Fast data fetching and web research
    # Use cases: "search for information", "fetch documentation", "find code examples"
    RETRIEVE:
      llm:
        model: openai/glm-4.5-air
        temperature: 0.0
        max_tokens: 16000
      prediction_strategy: react
      # Load RETRIEVE-specific seed prompt and demos
      signature_instructions: "prompt_optimization.prompts.seed_prompts.executor_retrieve_seed:EXECUTOR_RETRIEVE_PROMPT"
      demos: "prompt_optimization.prompts.seed_prompts.executor_retrieve_seed:EXECUTOR_RETRIEVE_DEMOS"
      agent_config:
        max_executions: 3

      # Web research and file tools
      toolkits:
        - class_name: MCPToolkit
          enabled: true
          toolkit_config:
            server_name: exa
            server_type: http
            url: https://mcp.exa.ai/mcp
            headers:
              Authorization: Bearer ${oc.env:EXA_API_KEY}
            transport_type: streamable
            use_storage: false
            tool_timeout: 60

        - class_name: FileToolkit
          enabled: true
          toolkit_config:
            enable_delete: false
            max_file_size: 10485760

    # CODE_INTERPRET: Code execution, analysis, and calculations
    # Use cases: "run this code", "analyze data", "calculate statistics"
    CODE_INTERPRET:
      llm:
        model: openai/glm-4.6
        temperature: 0.1
        max_tokens: 32000
      prediction_strategy: react
      # Load CODE_INTERPRET-specific seed prompt and demos
      signature_instructions: "prompt_optimization.prompts.seed_prompts.executor_code_seed:EXECUTOR_CODE_PROMPT"
      demos: "prompt_optimization.prompts.seed_prompts.executor_code_seed:EXECUTOR_CODE_DEMOS"
      agent_config:
        max_executions: 10

      # Code execution + file I/O + calculator
      toolkits:
        - class_name: MicroSandboxToolkit
          enabled: true
          toolkit_config:
            timeout: 300
            image: python
            cpus: 1
            memory: 1024

        - class_name: FileToolkit
          enabled: true
          toolkit_config:
            enable_delete: false
            max_file_size: 10485760

        - class_name: CalculatorToolkit
          enabled: true

    # WRITE: Content creation and documentation
    # Use cases: "write a report", "create documentation", "summarize findings"
    WRITE:
      llm:
        model: openai/glm-4.6
        temperature: 0.3
        max_tokens: 32000
      prediction_strategy: react
      # Load WRITE-specific seed prompt and demos
      signature_instructions: "prompt_optimization.prompts.seed_prompts.executor_write_seed:EXECUTOR_WRITE_PROMPT"
      demos: "prompt_optimization.prompts.seed_prompts.executor_write_seed:EXECUTOR_WRITE_DEMOS"
      agent_config:
        max_executions: 5

      # File I/O tools
      toolkits:
        - class_name: FileToolkit
          enabled: true
          toolkit_config:
            enable_delete: false
            max_file_size: 10485760

# Runtime configuration
runtime:
  max_depth: 4
  verbose: true
  enable_logging: true
  log_level: INFO
  timeout: 60

# Resilience
resilience:
  retry:
    enabled: true
    max_attempts: 3
    strategy: exponential_backoff
    base_delay: 1.0
    max_delay: 30.0

  circuit_breaker:
    enabled: true
    failure_threshold: 3
    recovery_timeout: 60.0
    half_open_max_calls: 2

  checkpoint:
    enabled: true
    storage_path: ${oc.env:ROMA_CHECKPOINT_PATH,.checkpoints}
    max_checkpoints: 10
    max_age_hours: 24.0
    compress_checkpoints: true
    verify_integrity: true

# Storage
storage:
  base_path: ${oc.env:STORAGE_BASE_PATH,/opt/sentient}
  max_file_size: 104857600 # 100MB

  postgres:
    enabled: ${oc.env:POSTGRES_ENABLED,true}
    connection_url: ${oc.env:DATABASE_URL,postgresql+asyncpg://localhost/roma_dspy}
    pool_size: 5
    max_overflow: 10

# Observability
observability:
  mlflow:
    enabled: ${oc.env:MLFLOW_ENABLED,false}
    tracking_uri: ${oc.env:MLFLOW_TRACKING_URI,http://mlflow:5000}
    experiment_name: ROMA-General-Agent
    log_traces: true
    log_compiles: true
    log_evals: true

# Logging
logging:
  level: ${oc.env:LOG_LEVEL,INFO}
  log_dir: ${oc.env:LOG_DIR,logs}
  console_format: detailed
  file_format: json
  serialize: true
  rotation: 500 MB
  retention: 30 days
  colorize: true
  backtrace: true
  diagnose: false
